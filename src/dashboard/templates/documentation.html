{% extends "content_page.html" %}

{% block title %}Documentation{% endblock %}
{% block page_title %}Documentation{% endblock %}

{% block header_title %}üìö Documentation & FAQ{% endblock %}

{% block breadcrumb_items %}
<a href="/documentation">Documentation</a>
{% endblock %}

{% block extra_css %}
<style>
    .faq-section {
        margin-bottom: 40px;
    }
    .faq-item {
        margin-bottom: 30px;
    }
    .faq-question {
        font-size: 18px;
        font-weight: 600;
        color: var(--accent-primary);
        margin-bottom: 10px;
    }
    .faq-answer {
        line-height: 1.6;
        color: var(--text-primary);
    }
    .faq-answer code {
        background: var(--bg-tertiary);
        padding: 2px 6px;
        border-radius: 3px;
        font-family: monospace;
    }
    .faq-answer ul {
        margin: 10px 0;
        padding-left: 30px;
    }
    .faq-answer ol {
        margin: 10px 0;
        padding-left: 30px;
    }
</style>
{% endblock %}

{% block main_content %}
<!-- Data Collection Methods -->
<div class="card">
    <h2>üìä Data Collection Methods</h2>

    <div class="faq-section">
        <h3>GitHub Metrics</h3>

        <div class="faq-item">
            <div class="faq-question">How are Pull Requests counted?</div>
            <div class="faq-answer">
                PRs are counted by creation date within the time window. Each PR created by a team member is counted once, regardless of merge status. Metrics include:
                <ul>
                    <li><strong>Total PRs</strong>: All PRs created in the period</li>
                    <li><strong>Merged PRs</strong>: PRs that were successfully merged</li>
                    <li><strong>Merge Rate</strong>: (Merged PRs / Total PRs) √ó 100</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Commits counted?</div>
            <div class="faq-answer">
                Commits are extracted from Pull Requests, not from the default branch. This ensures consistency:
                <ul>
                    <li>Only commits that are part of PRs are counted</li>
                    <li>Commits are attributed by GitHub username (not Git author name)</li>
                    <li>Each unique commit SHA is counted once (deduplicated if appearing in multiple PRs)</li>
                    <li>Team members must have commits ‚â• PRs (since 1 PR = at least 1 commit)</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Code Reviews counted?</div>
            <div class="faq-answer">
                Reviews are counted when submitted within the time window. Each review submission (comment, approval, or request changes) is counted once per PR. Multiple reviews on the same PR by the same person count separately.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Cycle Time?</div>
            <div class="faq-answer">
                Cycle Time measures the duration from PR creation to merge/close. It's calculated as:
                <ul>
                    <li><strong>For merged PRs</strong>: Time from PR creation to merge</li>
                    <li><strong>For closed PRs</strong>: Time from PR creation to close</li>
                    <li><strong>Average Cycle Time</strong>: Mean of all completed PRs in the period</li>
                </ul>
                Lower cycle times indicate faster code review and merge processes.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Lines of Code counted?</div>
            <div class="faq-answer">
                Lines Added and Lines Deleted are aggregated from all commits made by the person. These come from GitHub's diff statistics and include:
                <ul>
                    <li>All code changes (additions, deletions, modifications)</li>
                    <li>Configuration files, tests, and documentation</li>
                    <li>Generated code and dependencies if committed</li>
                </ul>
                Note: High line counts don't always mean higher productivity - quality matters more than quantity.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>Jira Metrics</h3>

        <div class="faq-item">
            <div class="faq-question">How are Jira issues counted?</div>
            <div class="faq-answer">
                Jira issues are fetched using JQL queries with date filtering:
                <ul>
                    <li><strong>Completed</strong>: Issues with resolution date within the configured time period</li>
                    <li><strong>In Progress</strong>: Issues currently unresolved (no resolution date)</li>
                    <li>Issues must be assigned to the team member's Jira username</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How does the system filter out noise from administrative updates?</div>
            <div class="faq-answer">
                The system uses optimized Jira queries to prevent bulk administrative operations (like mass label updates) from polluting metrics:
                <ul>
                    <li><strong>Query Logic</strong>: <code>assignee = "user" AND (created >= -90d OR resolved >= -90d OR (statusCategory != Done AND updated >= -90d))</code></li>
                    <li><strong>What it captures</strong>:
                        <ul>
                            <li>New issues created in last 90 days</li>
                            <li>Old issues resolved in last 90 days</li>
                            <li>Active work (statusCategory != Done) updated in last 90 days</li>
                        </ul>
                    </li>
                    <li><strong>What it filters out</strong>: Closed/Done tickets that only had administrative updates (labels, comments, attachments)</li>
                    <li><strong>Why it matters</strong>: A bulk label update on 5,000+ closed tickets won't pollute your metrics anymore</li>
                </ul>
                This ensures cleaner metrics and better query performance while maintaining 100% accuracy for actual work.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Throughput?</div>
            <div class="faq-answer">
                Throughput measures team velocity by counting issues completed per week over a 12-week period. It's calculated as:
                <ul>
                    <li>Total issues completed in last 12 weeks √∑ 12 = Weekly Average</li>
                    <li>Only counts issues with resolution dates in that period</li>
                    <li>Higher throughput indicates faster delivery</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is WIP (Work In Progress)?</div>
            <div class="faq-answer">
                WIP counts currently active issues:
                <ul>
                    <li><strong>WIP Count</strong>: Number of unresolved issues</li>
                    <li><strong>Average WIP Age</strong>: How long items have been in progress</li>
                    <li><strong>WIP by Status</strong>: Distribution across statuses (In Progress, Code Review, etc.)</li>
                </ul>
                Lower WIP counts and ages generally lead to faster completion.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What are Flagged/Blocked Issues?</div>
            <div class="faq-answer">
                Issues marked as blocked or flagged need attention:
                <ul>
                    <li>Fetched using team-specific Jira filters</li>
                    <li>Shows assignee and days blocked</li>
                    <li>Removing blockers maintains team flow</li>
                </ul>
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How are Bugs tracked?</div>
            <div class="faq-answer">
                Bug metrics track product quality:
                <ul>
                    <li><strong>Created</strong>: New bugs filed in the period</li>
                    <li><strong>Resolved</strong>: Bugs fixed in the period</li>
                    <li><strong>Net</strong>: Created - Resolved (negative is good)</li>
                </ul>
                Trend charts show 90-day history to identify patterns.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">What is Jira Cycle Time?</div>
            <div class="faq-answer">
                Jira Cycle Time measures issue resolution speed:
                <ul>
                    <li>Time from issue creation to resolution</li>
                    <li>Calculated in hours, displayed in hours</li>
                    <li>Average across all completed issues</li>
                </ul>
            </div>
        </div>
    </div>
</div>

<!-- DORA Metrics -->
<div class="card">
    <h2>üìà DORA Metrics</h2>

    <div class="faq-section">
        <h3>What are DORA Metrics?</h3>
        <div class="faq-answer">
            DORA (DevOps Research and Assessment) metrics are industry-standard measurements of software delivery performance. These four key metrics help teams understand their deployment velocity, stability, and incident response capabilities. Performance is classified into four levels: <strong>Elite</strong>, <strong>High</strong>, <strong>Medium</strong>, and <strong>Low</strong> based on research from thousands of organizations.
        </div>
    </div>

    <div class="faq-section">
        <h3>1. Deployment Frequency</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                How often your team deploys code to production. Measured as deployments per week.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                Uses <strong>Jira Fix Versions</strong> (not GitHub Releases) to track deployments:
                <ul>
                    <li>Releases matching pattern "Live - D/MMM/YYYY" are counted as production</li>
                    <li>Other patterns (e.g., "Staging") are filtered out</li>
                    <li>Deployments are counted within the 90-day window</li>
                    <li>Rate calculated as: (Total deployments / Days) √ó 7</li>
                </ul>
                A weekly trend chart shows deployment volume over time.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> More than 7 deployments per week (on-demand deployment)</li>
                    <li><strong>High:</strong> 1-7 deployments per week</li>
                    <li><strong>Medium:</strong> 1-4 deployments per month</li>
                    <li><strong>Low:</strong> Less than 1 deployment per month</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>2. Lead Time for Changes</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                Time from code commit to running in production. Measures how quickly changes move through your delivery pipeline.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                Uses a two-tier approach:
                <ol>
                    <li><strong>Jira-based (preferred):</strong> PR includes Jira issue key ‚Üí Issue mapped to Fix Version ‚Üí Time from PR merge to Fix Version release date</li>
                    <li><strong>Time-based (fallback):</strong> Time from PR merge to next deployment after merge</li>
                </ol>
                The median lead time across all PRs in the period is displayed. PRs without Jira issues use the fallback method.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 1 day</li>
                    <li><strong>High:</strong> 1-7 days</li>
                    <li><strong>Medium:</strong> 1-4 weeks</li>
                    <li><strong>Low:</strong> More than 4 weeks</li>
                </ul>
                A weekly trend chart shows median lead time over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>3. Change Failure Rate (CFR)</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                Percentage of deployments that cause production incidents. Indicates deployment stability and quality.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                <strong>Requires incident tracking setup (see below).</strong>
                <ul>
                    <li>Incidents are fetched from your Jira <code>incidents</code> filter</li>
                    <li>Incidents correlated to deployments using two methods:
                        <ul>
                            <li><strong>Direct:</strong> Incident has Fix Version matching deployment name</li>
                            <li><strong>Time-based:</strong> Incident created within 24 hours after deployment</li>
                        </ul>
                    </li>
                    <li>Calculation: (Failed deployments / Total deployments) √ó 100</li>
                </ul>
                Shows "‚Äî" if no incidents in period (which is excellent!).
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 15% (fewer than 1 in 7 deployments fail)</li>
                    <li><strong>High:</strong> 15-16%</li>
                    <li><strong>Medium:</strong> 16-30%</li>
                    <li><strong>Low:</strong> More than 30%</li>
                </ul>
                A weekly trend chart shows failure rate percentage over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>4. Mean Time to Recovery (MTTR)</h3>

        <div class="faq-item">
            <div class="faq-question">What does it measure?</div>
            <div class="faq-answer">
                How quickly your team recovers from production incidents. Measured as median time from incident creation to resolution.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">How is it calculated?</div>
            <div class="faq-answer">
                <strong>Requires incident tracking setup (see below).</strong>
                <ul>
                    <li>Incidents fetched from your Jira <code>incidents</code> filter</li>
                    <li>Resolution time = (Resolved timestamp - Created timestamp)</li>
                    <li>Median of all resolved incidents is displayed</li>
                    <li>P95 (95th percentile) also shown for outlier awareness</li>
                </ul>
                Shows "‚Äî" if no resolved incidents in period.
            </div>
        </div>

        <div class="faq-item">
            <div class="faq-question">Performance Levels</div>
            <div class="faq-answer">
                <ul>
                    <li><strong>Elite:</strong> Less than 1 hour</li>
                    <li><strong>High:</strong> 1-24 hours (less than 1 day)</li>
                    <li><strong>Medium:</strong> 1-7 days</li>
                    <li><strong>Low:</strong> More than 7 days</li>
                </ul>
                A weekly trend chart shows median recovery time over time.
            </div>
        </div>
    </div>

    <div class="faq-section">
        <h3>Setting Up Incident Tracking</h3>
        <div class="faq-answer">
            <p><strong>CFR and MTTR require incident tracking configuration.</strong></p>
            <p>To enable these metrics:</p>
            <ol>
                <li>Create a Jira filter for production incidents using JQL like:
                    <code style="display: block; margin: 10px 0; padding: 10px; background: var(--bg-tertiary);">
                        project IN (YOUR_PROJECTS)<br>
                        AND (issuetype = Incident OR priority IN (Blocker, Critical, High))<br>
                        AND labels IN (production, p1, sev1)<br>
                        AND (created >= -90d OR resolved >= -90d)
                    </code>
                </li>
                <li>Add the filter ID to your team configuration in <code>config/config.yaml</code>:
                    <code style="display: block; margin: 10px 0; padding: 10px; background: var(--bg-tertiary);">
                        jira:<br>
                        &nbsp;&nbsp;filters:<br>
                        &nbsp;&nbsp;&nbsp;&nbsp;incidents: 12345  # Your filter ID
                    </code>
                </li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
            </ol>
            <p>See <strong>README.md</strong> for detailed setup instructions and JQL examples.</p>
        </div>
    </div>

    <div class="faq-section">
        <h3>Overall DORA Performance Level</h3>
        <div class="faq-answer">
            Your team's overall DORA level is determined by the <strong>lowest-performing metric</strong> among the four. For example, if you have Elite deployment frequency but Low lead time, your overall level is Low. This encourages balanced improvement across all dimensions of software delivery performance.
        </div>
    </div>
</div>

<!-- Time Windows -->
<div class="card">
    <h2>‚è∞ Time Windows</h2>

    <div class="faq-item">
        <div class="faq-question">What time periods are used?</div>
        <div class="faq-answer">
            Different dashboards use different time windows:
            <ul>
                <li><strong>All Metrics (Team, Person, GitHub, Jira)</strong>: Fixed 90-day rolling window</li>
                <li><strong>Jira Throughput</strong>: Last 12 weeks (84 days) - calculated from completed issues</li>
                <li><strong>Jira Trends</strong>: Last 90 days for bugs/scope charts</li>
            </ul>
            <p style="margin-top: 10px;">
                <strong>Note:</strong> All metrics use a consistent 90-day window defined by the <code>DAYS_BACK</code> constant in the collection script.
                This ensures fair comparison across teams and individuals.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How often is data refreshed?</div>
        <div class="faq-answer">
            Data collection runs on a schedule:
            <ul>
                <li><strong>Automatic</strong>: Daily at 10:00 AM via launchd scheduler</li>
                <li><strong>Manual</strong>: Run <code>python collect_data.py</code></li>
                <li><strong>Duration</strong>: 15-30 minutes per collection</li>
                <li><strong>Cache</strong>: Stored in <code>data/metrics_cache.pkl</code></li>
            </ul>
            Dashboard shows cache timestamp at top of page.
        </div>
    </div>
</div>

<!-- API Information -->
<div class="card">
    <h2>üîå API Details</h2>

    <div class="faq-item">
        <div class="faq-question">Which GitHub API is used?</div>
        <div class="faq-answer">
            We use <strong>GitHub GraphQL API v4</strong> for efficiency:
            <ul>
                <li>Separate rate limit (5000 points/hour)</li>
                <li>50-70% fewer API calls than REST</li>
                <li>Single query fetches PRs + reviews + commits</li>
            </ul>
            See <code>src/collectors/github_graphql_collector.py</code> for implementation.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How is Jira accessed?</div>
        <div class="faq-answer">
            We use <strong>Jira REST API</strong> with Bearer token authentication:
            <ul>
                <li>Team-specific filter IDs configured per team</li>
                <li>JQL queries with date filtering</li>
                <li>SSL verification disabled for self-signed certificates</li>
            </ul>
            Use <code>python list_jira_filters.py</code> to discover filter IDs.
        </div>
    </div>
</div>

<!-- Common Questions -->
<div class="card">
    <h2>‚ùì Common Questions</h2>

    <div class="faq-item">
        <div class="faq-question">Why does someone show 0 commits but have PRs?</div>
        <div class="faq-answer">
            This shouldn't happen after the commit counting fix, but possible causes:
            <ul>
                <li>Git author name doesn't match GitHub username</li>
                <li>Commits were made by someone else but PR opened by this person</li>
                <li>Data collection issue - try re-running collection</li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">Why does someone show 0 Jira issues?</div>
        <div class="faq-answer">
            Possible reasons:
            <ul>
                <li>No Jira username mapping in config.yaml</li>
                <li>No issues resolved in the last 90 days</li>
                <li>Jira username doesn't match configuration</li>
                <li>Issues assigned to different username/account</li>
            </ul>
            Check Jira username mapping in <code>config/config.yaml</code>.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How do I add a new team member?</div>
        <div class="faq-answer">
            Update <code>config/config.yaml</code>:
            <ol>
                <li>Add GitHub username to <code>github.members</code> list</li>
                <li>Add Jira username to <code>jira.members</code> list</li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
                <li>Restart dashboard: <code>launchctl restart com.team-metrics.dashboard</code></li>
            </ol>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">Can I change the time window?</div>
        <div class="faq-answer">
            To change the collection time window:
            <ul>
                <li>Edit the <code>DAYS_BACK</code> constant in <code>collect_data.py</code> (line 19)</li>
                <li>Default is 90 days - change to any value (e.g., 30, 60, 180)</li>
                <li>Re-run data collection: <code>python collect_data.py</code></li>
                <li>Restart the dashboard to load new data</li>
            </ul>
            <p style="margin-top: 10px;">
                <strong>Note:</strong> All metrics (team, person, Jira) will use the same time window.
                There is no per-dashboard or per-person time window selection in the UI.
            </p>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">What does "Merge Rate" mean?</div>
        <div class="faq-answer">
            Merge Rate = (Merged PRs √∑ Total PRs) √ó 100%
            <ul>
                <li>100% = All PRs were merged</li>
                <li>&lt;100% = Some PRs were closed without merging</li>
                <li>Higher is generally better, but depends on team practices</li>
            </ul>
        </div>
    </div>
</div>

<!-- Technical Details -->
<div class="card">
    <h2>‚öôÔ∏è Technical Details</h2>

    <div class="faq-item">
        <div class="faq-question">Where is data stored?</div>
        <div class="faq-answer">
            All collected data is cached locally:
            <ul>
                <li><code>data/metrics_cache.pkl</code> - Main metrics cache (pickle format)</li>
                <li><code>logs/collect_data.log</code> - Collection logs</li>
                <li><code>logs/dashboard.log</code> - Dashboard logs</li>
            </ul>
            Cache includes: teams, persons, comparison data, and timestamp.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">How are metrics calculated?</div>
        <div class="faq-answer">
            Calculation pipeline:
            <ol>
                <li>Raw data collected as lists of dicts</li>
                <li>Converted to pandas DataFrames</li>
                <li>Aggregated by <code>MetricsCalculator</code> in <code>src/models/metrics.py</code></li>
                <li>Cached to disk as pickle</li>
                <li>Loaded by Flask and passed to Jinja templates</li>
            </ol>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">What happens if collection fails?</div>
        <div class="faq-answer">
            Dashboard continues using old cache until successful collection:
            <ul>
                <li>Check <code>logs/collect_data.log</code> for errors</li>
                <li>Verify API tokens in <code>config/config.yaml</code></li>
                <li>Check API rate limits (GitHub/Jira)</li>
                <li>Manually trigger: <code>python collect_data.py</code></li>
            </ul>
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üß≠ How do I navigate between different views?</div>
        <div class="faq-answer">
            Use the hamburger menu (‚ò∞) in the top-right corner of any page. The menu provides:
            <ul>
                <li><strong>üè† Home:</strong> Return to the main teams overview</li>
                <li><strong>üìö Documentation:</strong> View this help page</li>
                <li><strong>üåì Theme Toggle:</strong> Switch between light and dark modes</li>
            </ul>
            The menu automatically closes when you select an option or click outside of it.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üé® What do the chart colors mean?</div>
        <div class="faq-answer">
            Charts use a consistent semantic color system across all pages:
            <ul>
                <li><strong style="color: #e74c3c;">üî¥ Red:</strong> Items created or added (bugs filed, scope added)</li>
                <li><strong style="color: #2ecc71;">üü¢ Green:</strong> Items resolved or completed (bugs fixed, scope delivered)</li>
                <li><strong style="color: #3498db;">üîµ Blue:</strong> Net difference (created minus resolved)</li>
            </ul>
            <strong>Trend Charts:</strong> Top panel shows created vs resolved, bottom panel shows net difference.<br>
            <strong>Tip:</strong> Positive net difference (blue trending up) indicates backlog growth; negative indicates backlog reduction.
        </div>
    </div>

    <div class="faq-item">
        <div class="faq-question">üë• How do I compare team members?</div>
        <div class="faq-answer">
            View side-by-side comparison of all team members:
            <ul>
                <li>Navigate to a team dashboard (click on team card from homepage)</li>
                <li>Open the hamburger menu (‚ò∞ icon in the breadcrumbs bar)</li>
                <li>Click "üë• Compare Members"</li>
                <li>See leaderboard with top performers (ü•áü•àü•â badges)</li>
                <li>View detailed metrics table with best values highlighted in green</li>
                <li>Compare PRs, reviews, commits, Jira metrics, and more</li>
            </ul>

            <h3 style="margin-top: 30px;">Performance Scoring System</h3>
            <p>
                The dashboard uses a <strong>composite performance scoring system</strong> to rank teams and individuals.
                Scores range from <strong>0-100</strong> (higher is better) and are calculated using min-max normalization
                across all teams/members.
            </p>

            <h4>Metric Weights:</h4>
            <table style="width: 100%; margin: 20px 0; border-collapse: collapse;">
                <thead>
                    <tr style="background: var(--bg-tertiary);">
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Metric</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Weight</th>
                        <th style="padding: 10px; text-align: left; border: 1px solid var(--border-color);">Description</th>
                    </tr>
                </thead>
                <tbody>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">PRs Created</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Pull requests authored</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Reviews Given</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Code reviews provided to others</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Commits</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">15%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Total commits authored</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Cycle Time</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">15%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">PR merge time (lower is better, inverted)</td>
                    </tr>
                    <tr>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Jira Completed</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">20%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Jira issues resolved</td>
                    </tr>
                    <tr style="background: var(--bg-secondary);">
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Merge Rate</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">10%</td>
                        <td style="padding: 10px; border: 1px solid var(--border-color);">Percentage of PRs successfully merged</td>
                    </tr>
                </tbody>
            </table>

            <h4>Key Features:</h4>
            <ul style="margin-top: 15px;">
                <li><strong>Cycle Time Inversion:</strong> Lower cycle times score higher (faster PR merges are better)</li>
                <li><strong>Team Size Normalization:</strong> Divides volume metrics by team size for fair per-capita comparison</li>
                <li><strong>Consistent Algorithm:</strong> Same scoring logic for both team and person comparisons</li>
                <li><strong>Visual Rankings:</strong> Teams/members displayed with scores, ranks, and badges (ü•áü•àü•â)</li>
            </ul>

            <p style="margin-top: 20px;">
                <strong>Where you see it:</strong>
            </p>
            <ul>
                <li>Team Comparison page: Overall Performance card with scores per team</li>
                <li>Team Member Comparison: Top Performers leaderboard with rankings</li>
            </ul>
        </div>
    </div>
</div>
{% endblock %}
